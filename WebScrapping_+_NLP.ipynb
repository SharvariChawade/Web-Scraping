{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SharvariChawade/Web-Scraping-NLP/blob/main/WebScrapping_%2B_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import os"
      ],
      "metadata": {
        "id": "SGYfIAlv-8H2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/input.csv')"
      ],
      "metadata": {
        "id": "17prn-Xy--b3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 37\n",
        "\n",
        "for url in (df['URL']):\n",
        "  path = '/content/allscrapedfiles'\n",
        "  html_text = requests.get(url,headers={\"User-Agent\": \"XY\"}).text\n",
        "  soup = BeautifulSoup(html_text,'lxml')\n",
        "  article_title = soup.find('h1',class_ = 'entry-title')\n",
        "  if(article_title == None):\n",
        "    i+=1\n",
        "    continue\n",
        "  else:\n",
        "    article_texts = soup.find_all('p')\n",
        "    content = ''\n",
        "    for article_text in article_texts:\n",
        "      content = content + article_text.text +'\\n'\n",
        "    temp = f'id{i}.txt'\n",
        "    path = os.path.join(path,temp)\n",
        "    f = open(path, \"w\")\n",
        "    f.write(f'{article_title.text}\\n{content}')\n",
        "    f.close()\n",
        "    i+=1"
      ],
      "metadata": {
        "id": "hSGFfw1zBMY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmEEkA0u7ml_",
        "outputId": "b56383bd-a68d-41f1-fc8a-5620cad90ad5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import re\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('/content/negative-words.txt','r',encoding = \"ISO-8859-1\")\n",
        "negativewords = f.read()\n",
        "negativewords = list(negativewords.split('\\n'))\n",
        "\n",
        "f = open('/content/positive-words.txt','r',encoding = \"ISO-8859-1\")\n",
        "positivewords = f.read()\n",
        "positivewords = list(positivewords.split('\\n'))\n",
        "\n",
        "stop_words = []\n",
        "stopwords = ''\n",
        "for file in os.listdir('/content'):\n",
        "  if 'StopWords' in file:\n",
        "    path = os.path.join('/content',file)\n",
        "    f = open(path,'r',encoding = \"ISO-8859-1\")\n",
        "    stopword = f.read().lower()\n",
        "    stopwords += stopword\n",
        "\n",
        "stop_words = list(stopwords.split('\\n'))\n",
        "\n",
        "positive_words = []\n",
        "negative_words = []\n",
        "for w in positivewords:\n",
        "  if w not in stop_words:\n",
        "    positive_words.append(w)\n",
        "\n",
        "for w in negativewords:\n",
        "  if w not in stop_words:\n",
        "    negative_words.append(w)\n"
      ],
      "metadata": {
        "id": "tnn68qNihzpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = [['POSITIVE SCORE','NEGATIVE SCORE','POLARITY SCORE','SUBJECTIVITY SCORE','AVG SENTENCE LENGTH','PERCENTAGE OF COMPLEX WORDS','FOG INDEX','AVG NUMBER OF WORDS PER SENTENCE','COMPLEX WORD COUNT','WORD COUNT','SYLLABLE PER WORD','PERSONAL PRONOUNS','AVG WORD LENGTH']]"
      ],
      "metadata": {
        "id": "OCyRSoOq7lHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(37, 151):\n",
        "  if (i == 44 ):\n",
        "    df1.append(['NaN','NaN','NaN','NaN','NaN','NaN','NaN','NaN','NaN','NaN','NaN','NaN','NaN'])\n",
        "    continue\n",
        "  elif (i == 57 ):\n",
        "    df1.append(['NaN','NaN','NaN','NaN','NaN','NaN','NaN','NaN','NaN','NaN','NaN','NaN','NaN'])\n",
        "    continue\n",
        "  elif (i == 144):\n",
        "    df1.append(['NaN','NaN','NaN','NaN','NaN','NaN','NaN','NaN','NaN','NaN','NaN','NaN','NaN'])\n",
        "    continue\n",
        "  else:\n",
        "    f = open(f'/content/allscrapedfiles/id{i}.txt','r')\n",
        "    text = f.read()\n",
        "\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    text_words = []\n",
        "\n",
        "    for i in range (len(sentences)):\n",
        "      review = re.sub('[^a-zA-Z]',' ',sentences[i])\n",
        "      review = review.lower()\n",
        "      review = review.split()\n",
        "      review = [lemmatizer.lemmatize(word) for word in review if not word in stop_words]\n",
        "      for w in review:\n",
        "        text_words.append(w)\n",
        "\n",
        "    pos = 0\n",
        "    neg = 0\n",
        "\n",
        "    for w in text_words:\n",
        "      if w in positive_words:\n",
        "        pos += 1\n",
        "\n",
        "    for w in text_words:\n",
        "      if w in negative_words:\n",
        "        neg += 1\n",
        "\n",
        "    polarity = (pos - neg)/(pos+neg)+0.000001\n",
        "    subjectivity = (pos+neg)/((len(text_words))+0.000001)\n",
        "\n",
        "    complex_words = []\n",
        "    for w in text_words:\n",
        "      if len(w) > 2:\n",
        "        complex_words.append(w)\n",
        "\n",
        "    average_sentence_len = len(text_words)/len(sentences)\n",
        "    percent_complex_words = len(complex_words)/len(text_words)\n",
        "    fog_index = 0.4*(average_sentence_len+percent_complex_words)\n",
        "    avg_no_of_word = len(text_words)/len(sentences)\n",
        "    complex_count = len(complex_words)\n",
        "\n",
        "    word_count_list = []\n",
        "    from nltk.corpus import stopwords\n",
        "    for w in text_words:\n",
        "      if w not in set(stopwords.words('english')):\n",
        "        word_count_list.append(w)\n",
        "\n",
        "    word_count = len(word_count_list)\n",
        "\n",
        "    syllable = 0\n",
        "    text = text.replace('\\n',' ')\n",
        "    list_text = text.split(' ')\n",
        "\n",
        "    for w in list_text:\n",
        "      for i in range(len(w)):\n",
        "        if (w[i] == 'a' or w[i] == 'e' or w[i] == 'i' or w[i] == 'o' or w[i] == 'u'):\n",
        "          if(i != len(w) - 1):\n",
        "            if(w[i] == 'e' and w[i+1] != 's' and w[i+1] != 'd'):\n",
        "              syllable += 1\n",
        "\n",
        "    personal_pronouns = 0\n",
        "\n",
        "    for i in sentences:\n",
        "      if('the us' in i):\n",
        "        i.replace('the us', \"THE US\")\n",
        "      l = []\n",
        "      l = re.findall(\"I,|we,|my,|ours,|us\",i)\n",
        "      personal_pronouns += len(l)\n",
        "\n",
        "\n",
        "    chars_in_text = text.replace(' ','')\n",
        "    avg_word_len = len(chars_in_text)/len(list_text)\n",
        "\n",
        "    df1.append([pos,neg,polarity,subjectivity,average_sentence_len,percent_complex_words,fog_index,avg_no_of_word,complex_count,word_count,syllable,personal_pronouns,avg_word_len])"
      ],
      "metadata": {
        "id": "VbktMPGDg98f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        " \n",
        "# create an array\n",
        "a = numpy.array(df1)\n",
        "#text = \"POSITIVE SCORE,NEGATIVE SCORE,POLARITY SCORE,SUBJECTIVITY SCORE,AVG SENTENCE LENGTH,PERCENTAGE OF COMPLEX WORDS,FOG INDEX,AVG NUMBER OF WORDS PER SENTENCE,COMPLEX WORD COUNT,WORD COUNT,SYLLABLE PER WORD,PERSONAL PRONOUNS,AVG WORD LENGTH\\n\"\n",
        "# save array into csv file\n",
        "rows = [\"{},{},{},{},{},{},{},{},{},{},{},{},{}\".format(a,b,c,d,e,f,g,h,i,j,k,l,m) for a,b,c,d,e,f,g,h,i,j,k,l,m in a]\n",
        "text = \"\\n\".join(rows)\n",
        " \n",
        "with open('/content/df1.csv', 'w') as f:\n",
        "    f.write(text)"
      ],
      "metadata": {
        "id": "xy5Ffw4OFtDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_1 = pd.read_csv('/content/df1.csv')"
      ],
      "metadata": {
        "id": "xeKA3c3IKkUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = pd.concat([df, df_1], axis = 1)"
      ],
      "metadata": {
        "id": "_wbXtmImLAQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output.to_csv('/content/output.csv')"
      ],
      "metadata": {
        "id": "kX2ZIfHOLYCa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}